proj_data <- read_tsv("airline.tsv")
#print summary of data
summary(proj_data)
#model Population Based on Assets
assets_model <- lm(Pop~Assets, data = proj_data)
#model Population Based on Hours
hours_model <- lm(Pop~Hours, data = proj_data)
#model Population Based on Miles
miles_model <- lm(Pop~Miles, data = proj_data)
#Plot Population to Hours of Flight Time
proj_data |> ggplot(aes(Hours, Pop)) + geom_point(cex = 4) + labs(x = "Hours of Flight Time", y = "Population(Thousands)") + theme(text = element_text(size = 10))
+ geom_smooth(formula = y ~ x, method = "lm", se = FALSE)
#print summary of data
summary(proj_data)
#model Population Based on Assets
assets_model <- lm(Pop~Assets, data = proj_data)
#model Population Based on Hours
hours_model <- lm(Pop~Hours, data = proj_data)
#model Population Based on Miles
miles_model <- lm(Pop~Miles, data = proj_data)
#Plot Population to Hours of Flight Time
proj_data |> ggplot(aes(Hours, Pop)) + geom_point(cex = 4) + labs(x = "Hours of Flight Time", y = "Population(Thousands)") + theme(text = element_text(size = 10)) +
geom_smooth(formula = y ~ x, method = "lm", se = FALSE)
#Plot Population to Total Assets
proj_data |> ggplot(aes(Assets, Pop)) + geom_point(cex = 4) + labs(x = "Total Assets(Per 100k)",y = "Population(Thousands)") +
theme(text = element_text(size = 10)) + geom_smooth(formula = y ~ x, method = "lm", se = FALSE)
#Plot Popultion to Miles
proj_data |> ggplot(aes(Miles, Pop)) + geom_point(cex = 4) + labs(x = "Miles Travelled per flight",y = "Population(Thousands)") + theme(text = element_text(size = 10)) +
geom_smooth(formula = y ~ x, method = "lm", se = FALSE)
#Combo model of both hours and Miles
combo_model <- lm(Pop~Hours+Miles, data = proj_data)
#Summary block for each model
summary(hours_model)
summary(assets_model)
summary(miles_model)
summary(combo_model)
plot_ly(proj_data, x = ~Hours, y = ~Miles, z = ~Pop, type = 'scatter3d', mode = 'markers') %>%
add_trace(x = ~Hours, y = ~Miles, z = fitted(combo_model), type = 'scatter3d', mode = 'lines', line = list(color = 'red'))
library(plotly)
install.packages("plotly")
library(plotly)
plot_ly(proj_data, x = ~Hours, y = ~Miles, z = ~Pop, type = 'scatter3d', mode = 'markers') %>%
add_trace(x = ~Hours, y = ~Miles, z = fitted(combo_model), type = 'scatter3d', mode = 'lines', line = list(color = 'red'))
proj_data |> print(n=31)
summary(proj_data)
#model Population Based on Assets
assets_model <- lm(Pop~Assets, data = proj_data)
#model Population Based on Hours
hours_model <- lm(Pop~Hours, data = proj_data)
#model Population Based on Miles
miles_model <- lm(Pop~Miles, data = proj_data)
#Plot Population to Hours of Flight Time
proj_data |> ggplot(aes(Hours, Pop)) + geom_point(cex = 4) + labs(x = "Hours of Flight Time", y = "Population(Thousands)") + theme(text = element_text(size = 10)) +
geom_smooth(formula = y ~ x, method = "lm", se = FALSE) + geom_text_repel(aes(label = rownames(proj_data)), size = 3, box.padding = 0.5)
proj_data |> ggplot(aes(Hours, Pop)) + geom_point(cex = 4) + labs(x = "Hours of Flight Time", y = "Population(Thousands)") + theme(text = element_text(size = 10)) +
geom_smooth(formula = y ~ x, method = "lm", se = FALSE) + geom_text(aes(label = rownames(proj_data)), size = 3, box.padding = 0.5)
#Plot Population to Total Assets
proj_data |> ggplot(aes(Assets, Pop)) + geom_point(cex = 4) + labs(x = "Total Assets(Per 100k)",y = "Population(Thousands)") +
theme(text = element_text(size = 10)) + geom_smooth(formula = y ~ x, method = "lm", se = FALSE) + geom_text(aes(label = rownames(proj_data)), size = 3, box.padding = 0.5)
#Plot Popultion to Miles
proj_data |> ggplot(aes(Miles, Pop)) + geom_point(cex = 4) + labs(x = "Miles Travelled per flight",y = "Population(Thousands)") + theme(text = element_text(size = 10)) +
geom_smooth(formula = y ~ x, method = "lm", se = FALSE) + geom_textl(aes(label = rownames(proj_data)), size = 3, box.padding = 0.5)
#Plot Population to Hours of Flight Time
proj_data |> ggplot(aes(Hours, Pop)) + geom_point(cex = 4) + labs(x = "Hours of Flight Time", y = "Population(Thousands)") + theme(text = element_text(size = 10)) +
geom_smooth(formula = y ~ x, method = "lm", se = FALSE) + geom_text(aes(label = rownames(proj_data)), size = 3, box.padding = 0.5)
#Plot Population to Total Assets
proj_data |> ggplot(aes(Assets, Pop)) + geom_point(cex = 4) + labs(x = "Total Assets(Per 100k)",y = "Population(Thousands)") +
theme(text = element_text(size = 10)) + geom_smooth(formula = y ~ x, method = "lm", se = FALSE) + geom_text(aes(label = rownames(proj_data)), size = 3, box.padding = 0.5)
#Plot Popultion to Miles
proj_data |> ggplot(aes(Miles, Pop)) + geom_point(cex = 4) + labs(x = "Miles Travelled per flight",y = "Population(Thousands)") + theme(text = element_text(size = 10)) +
geom_smooth(formula = y ~ x, method = "lm", se = FALSE) + geom_text(aes(label = rownames(proj_data)), size = 3, box.padding = 0.5)
proj_data |> ggplot(aes(Miles, Pop)) + geom_point(cex = 4) + labs(x = "Miles Travelled per flight",y = "Population(Thousands)") + theme(text = element_text(size = 10)) +
geom_smooth(formula = y ~ x, method = "lm", se = FALSE) + geom_text(aes(label = rownames(proj_data)), size = 3)
proj_data |> ggplot(aes(Miles, Pop)) + geom_point(cex = 4) + labs(x = "Miles Travelled per flight",y = "Population(Thousands)") + theme(text = element_text(size = 10)) +
geom_smooth(formula = y ~ x, method = "lm", se = FALSE) + geom_text(aes(label = rownames(proj_data)), size = 8)
#Plot Population to Hours of Flight Time
proj_data |> ggplot(aes(Hours, Pop)) + geom_point(cex = 4) + labs(x = "Hours of Flight Time", y = "Population(Thousands)") + theme(text = element_text(size = 10)) +
geom_smooth(formula = y ~ x, method = "lm", se = FALSE)
#Plot Population to Total Assets
proj_data |> ggplot(aes(Assets, Pop)) + geom_point(cex = 4) + labs(x = "Total Assets(Per 100k)",y = "Population(Thousands)") +
theme(text = element_text(size = 10)) + geom_smooth(formula = y ~ x, method = "lm", se = FALSE)
#Plot Popultion to Miles
proj_data |> ggplot(aes(Miles, Pop)) + geom_point(cex = 4) + labs(x = "Miles Travelled per flight",y = "Population(Thousands)") + theme(text = element_text(size = 10)) +
geom_smooth(formula = y ~ x, method = "lm", se = FALSE)
#model Population Based on Assets
assets_model <- lm(Pop~Assets, data = proj_data)
#model Population Based on Hours
hours_model <- lm(Pop~Hours, data = proj_data)
#model Population Based on Miles
miles_model <- lm(Pop~Miles, data = proj_data)
#Plot Population to Hours of Flight Time
proj_data |> ggplot(aes(Hours, Pop)) + geom_point(cex = 4) + labs(x = "Hours of Flight Time", y = "Population(Thousands)") + theme(text = element_text(size = 10)) +
geom_smooth(formula = y ~ x, method = "lm", se = FALSE)
#Plot Population to Total Assets
proj_data |> ggplot(aes(Assets, Pop)) + geom_point(cex = 4) + labs(x = "Total Assets(Per 100k)",y = "Population(Thousands)") +
theme(text = element_text(size = 10)) + geom_smooth(formula = y ~ x, method = "lm", se = FALSE)
#Plot Popultion to Miles
proj_data |> ggplot(aes(Miles, Pop)) + geom_point(cex = 4) + labs(x = "Miles Travelled per flight",y = "Population(Thousands)") + theme(text = element_text(size = 10)) +
geom_smooth(formula = y ~ x, method = "lm", se = FALSE)
#Combo model of both hours and Miles
combo_model <- lm(Pop~Hours+Miles, data = proj_data)
#Summary block for each model
summary(hours_model)
summary(assets_model)
summary(miles_model)
summary(combo_model)
proj_data |> print(n=32)
# Read in data, look at the variables and create a training and test set
file <- "binary_class_dataset.txt"
set <- read.table(file, header = TRUE, sep = "\t")
names(set)
split <- .65
set["rand"] <- runif(nrow(set))
train <- set[(set$rand <= split), ]
test <- set[(set$rand > split), ]
set$Y <- set$Y_BUY
##########################################################
########### R FUNCTIONS ##########
##########################################################
library(mgcv)
# GAM Smoothed plot
plotrel <- function(x, y, b, title) {
# Produce a GAM smoothed representation of the data
g <- gam(as.formula("y ~ x"), family = "binomial", data = set)
xs <- seq(min(x), max(x), length = 200)
p <- predict(g, newdata = data.frame(x = xs), type = "response")
# Now get empirical estimates (and discretize if non discrete)
if (length(unique(x)) > b) {
div <- floor(max(x) / b)
x_b <- floor(x / div) * div
c <- table(x_b, y)
}
else { c <- table(x, y) }
pact <- c[ , 2]/(c[ , 1]+c[, 2])
cnt <- c[ , 1]+c[ , 2]
xd <- as.integer(rownames(c))
plot(xs, p, type="l", main=title,ylab = "P(Conversion | Ad, X)", xlab="X")
points(xd, pact, type="p", col="red")
rug(x+runif(length(x)))
}
library(plyr)
# wMAE plot and calculation
getmae <- function(p, y, b, title, doplot) {
# Normalize to interval [0,1]
max_p <- max(p)
p_norm <- p / max_p
# break up to b bins and rescale
bin <- max_p * floor(p_norm * b) / b
d <- data.frame(bin, p, y)
t <- table(bin)
summ <- ddply(d, .(bin), summarise, mean_p = mean(p), mean_y = mean(y))
fin <- data.frame(bin = summ$bin, mean_p = summ$mean_p, mean_y = summ$mean_y, t)
# Get wMAE
num = 0
den = 0
for (i in c(1:nrow(fin))) {
num <- num + fin$Freq[i] * abs(fin$mean_p[i] - fin$mean_y[i])
den <- den + fin$Freq[i]
}
wmae <- num / den
if (doplot == 1) {
plot(summ$bin, summ$mean_p, type = "p",
main = paste(title," MAE =", wmae),
col = "blue", ylab = "P(C | AD, X)",
xlab = "P(C | AD, X)")
points(summ$bin, summ$mean_y, type = "p", col = "red")
rug(p)
}
return(wmae)
}
install.packages("ROCR",dep=T)
library(ROCR)
get_auc <- function(ind, y) {
pred <- prediction(ind, y)
perf <- performance(pred, 'auc', fpr.stop = 1)
auc <- as.numeric(substr(slot(perf, "y.values"), 1, 8), double)
return(auc)
}
# Get X-Validated performance metrics for a given feature set
getxval <- function(vars, data, folds, mae_bins) {
# assign each observation to a fold
data["fold"] <- floor(runif(nrow(data)) * folds) + 1
auc <- c()
wmae <- c()
fold <- c()
# make a formula object
f = as.formula(paste("Y", "~", paste(vars, collapse = "+")))
for (i in c(1:folds)) {
train <- data[(data$fold != i), ]
test <- data[(data$fold == i), ]
mod_x <- glm(f, data=train, family = binomial(logit))
p <- predict(mod_x, newdata = test, type = "response")
# Get wMAE
wmae <- c(wmae, getmae(p, test$Y, mae_bins,"dummy", 0))
fold <- c(fold, i)
auc <- c(auc, get_auc(p, test$Y))
}
return(data.frame(fold, wmae, auc))
}
###############################################################
########## MAIN: MODELS AND PLOTS ##########
###############################################################
# Now build a model on all variables and look at coefficients and model fit
vlist <- c("AT_BUY_BOOLEAN", "AT_FREQ_BUY",
"AT_FREQ_LAST24_BUY",
"AT_FREQ_LAST24_SV", "AT_FREQ_SV", "EXPECTED_TIME_BUY",
"EXPECTED_TIME_SV", "LAST_BUY", "LAST_SV", "NUM_CHECKINS")
f = as.formula(paste("Y_BUY", "~" , paste(vlist,collapse = "+")))
fit <- glm(f, data = train, family = binomial(logit))
summary(fit)
# Get performance metrics on each variable
vlist <- c("AT_BUY_BOOLEAN", "AT_FREQ_BUY",
"AT_FREQ_LAST24_BUY",
"AT_FREQ_LAST24_SV", "AT_FREQ_SV", "EXPECTED_TIME_BUY",
"EXPECTED_TIME_SV", "LAST_BUY", "LAST_SV", "NUM_CHECKINS")
# Create empty vectors to store the performance/evaluation metrics
auc_mu <- c()
auc_sig <- c()
mae_mu <- c()
mae_sig <- c()
for (i in c(1:length(vlist))) {
a <- getxval(c(vlist[i]), set, 10, 100)
auc_mu <- c(auc_mu, mean(a$auc))
auc_sig <- c(auc_sig, sd(a$auc))
mae_mu <- c(mae_mu, mean(a$wmae))
mae_sig <- c(mae_sig, sd(a$wmae))
}
univar <- data.frame(vlist, auc_mu, auc_sig, mae_mu, mae_sig)
# Get MAE plot on single variable - use holdout group for evaluation
set <- read.table(file, header = TRUE, sep = "\t")
names(set)
split<-.65
set["rand"] <- runif(nrow(set))
train <- set[(set$rand <= split), ]
test <- set[(set$rand > split), ]
set$Y <- set$Y_BUY
fit <- glm(Y_BUY ~ NUM_CHECKINS, data = train,family = binomial(logit))
y <- test$Y_BUY
p <- predict(fit, newdata = test, type = "response")
getmae(p,y,50,"NUM_CHECKINS",1)
# Greedy Forward Selection
rvars <- c("LAST_SV", "AT_FREQ_SV", "AT_FREQ_BUY",
"AT_BUY_BOOLEAN", "LAST_BUY", "AT_FREQ_LAST24_SV",
"EXPECTED_TIME_SV", "NUM_CHECKINS",
"EXPECTED_TIME_BUY", "AT_FREQ_LAST24_BUY")
# Create empty vectors
auc_mu <- c()
auc_sig <- c()
mae_mu <- c()
mae_sig <- c()
for (i in c(1:length(rvars))) {
vars <- rvars[1:i]
vars
a <- getxval(vars, set, 10, 100)
auc_mu <- c(auc_mu, mean(a$auc))
auc_sig <- c(auc_sig, sd(a$auc))
mae_mu <- c(mae_mu, mean(a$wmae))
mae_sig <- c(mae_sig, sd(a$wmae))
}
kvar<-data.frame(auc_mu, auc_sig, mae_mu, mae_sig)
# Plot 3 AUC Curves
y <- test$Y_BUY
fit <- glm(Y_BUY~LAST_SV, data=train, family = binomial(logit))
p1 <- predict(fit, newdata=test, type="response")
fit <- glm(Y_BUY~LAST_BUY, data=train, family = binomial(logit))
p2 <- predict(fit, newdata=test, type="response")
fit <- glm(Y_BUY~NUM_CHECKINS, data=train, family = binomial(logit))
p3 <- predict(fit, newdata=test,type="response")
pred <- prediction(p1,y)
perf1 <- performance(pred,'tpr','fpr')
pred <- prediction(p2,y)
perf2 <- performance(pred,'tpr','fpr')
pred <- prediction(p3,y)
perf3 <- performance(pred,'tpr','fpr')
plot(perf1, color="blue", main="LAST_SV (blue), LAST_BUY (red), NUM_CHECKINS (green)")
plot(perf2, col="red", add=TRUE)
plot(perf3, col="green", add=TRUE)
plot(perf3, col="green", add=TRUE)
#Samuel Hartmann, Brence Moore, Logan McDavid, Nathan Lamb, Caleb Smith
#load tidyverse and input tsv
library(tidyverse)
proj_data <- read_tsv("airline.tsv")
#print summary of data
summary(proj_data)
#model Population Based on Assets
assets_model <- lm(Pop~Assets, data = proj_data)
#model Population Based on Hours
hours_model <- lm(Pop~Hours, data = proj_data)
#model Population Based on Miles
miles_model <- lm(Pop~Miles, data = proj_data)
#Plot Population to Hours of Flight Time
proj_data |> ggplot(aes(Hours, Pop)) + geom_point(cex = 4) + labs(x = "Hours of Flight Time", y = "Population(Thousands)") + theme(text = element_text(size = 10)) + geom_smooth(formula = y ~ x, method = "lm", se = FALSE)
#Plot Population to Total Assets
proj_data |> ggplot(aes(Assets, Pop)) + geom_point(cex = 4) + labs(x = "Total Assets(Per 100k)",y = "Population(Thousands)") + theme(text = element_text(size = 10)) + geom_smooth(formula = y ~ x, method = "lm", se = FALSE)
#Plot Popultion to Miles
proj_data |> ggplot(aes(Miles, Pop)) + geom_point(cex = 4) + labs(x = "Miles Travelled per flight",y = "Population(Thousands)") + theme(text = element_text(size = 10)) + geom_smooth(formula = y ~ x, method = "lm", se = FALSE)
#Combo model of both hours and Miles
combo_model <- lm(Pop~Hours+Miles, data = proj_data)
#Summary block for each model
summary(hours_model)
summary(assets_model)
summary(miles_model)
summary(combo_model)
install.packages("languageserver")
#Team 3:Samuel Hartmann, Logan McDavid, Brence Moore, Nathan Lamb
#Code for reading in and sorting data written by Samuel
#load tidyverse and plays.csv
library(tidyverse)
library(tidymodels)
library(discrim)
master_data <- read.csv("plays.csv")
setwd("~/GitHub/Team-3-NFL-Data-Bowl")
setwd("~/GitHub/Team-3-NFL-Data-Bowl")
#Team 3:Samuel Hartmann, Logan McDavid, Brence Moore, Nathan Lamb
#Code for reading in and sorting data written by Samuel
#load tidyverse and plays.csv
library(tidyverse)
library(tidymodels)
library(discrim)
master_data <- read.csv("plays.csv")
#Select only gameId, playID, quarter, down, yardsToGo, defensiveTeam, gameClock, pff_passCoverage, and pff_manZone
working_data <- master_data |> select(gameId, playId, quarter, down, yardsToGo, defensiveTeam, gameClock, pff_passCoverage, pff_manZone)
summary(working_data)
#Review Na and Other Values
Na_data <- master_data |> filter(is.na(pff_manZone))
Other_data <- master_data |> filter(pff_manZone == 'Other')
print(Na_data$playDescription)
print(Other_data$playDescription)
#remove row with NA values
cleaned_data <- na.omit(working_data)
#remove other values
cleaned_master <- cleaned_data |> filter(pff_manZone != 'Other')
summary(cleaned_master)
#get counts of each defensive alignment
count_defs <- cleaned_master |> count(pff_passCoverage)
print(count_defs)
#clean Cover-3 of variants and Cover-1 of Cover-1 Double
ready_master <- cleaned_master |> mutate(pff_passCoverage = case_when(
pff_passCoverage == "Cover-3 Cloud Left" ~ "Cover-3",
pff_passCoverage == "Cover-3 Cloud Right" ~ "Cover-3",
pff_passCoverage == "Cover-3 Double Cloud" ~ "Cover-3",
pff_passCoverage == "Cover-1 Double" ~ "Cover-1",
TRUE ~ pff_passCoverage))
#get counts of each defensive alignment after cleaning
count_defs <- ready_master |> count(pff_passCoverage)
print(count_defs)
#get list of team Ids
team_Ids <- unique(ready_master$defensiveTeam)
print(team_Ids)
#AFC East
BUF_data <- ready_master  |> filter(defensiveTeam == 'BUF')
MIA_data <- ready_master  |> filter(defensiveTeam == 'MIA')
NJY_data <- ready_master  |> filter(defensiveTeam == 'NYJ')
NE_data <- ready_master  |> filter(defensiveTeam == 'NE')
#AFC West
KC_data <- ready_master  |> filter(defensiveTeam == 'KC')
LAC_data <- ready_master  |> filter(defensiveTeam == 'LAC')
DEN_data <- ready_master  |> filter(defensiveTeam == 'DEN')
LV_data <- ready_master  |> filter(defensiveTeam == 'LV')
#AFC North
PIT_data <- ready_master  |> filter(defensiveTeam == 'PIT')
BAL_data <- ready_master  |> filter(defensiveTeam == 'BAL')
CIN_data <- ready_master  |> filter(defensiveTeam == 'CIN')
CLE_data <- ready_master  |> filter(defensiveTeam == 'CLE')
#AFC South
HOU_data <- ready_master  |> filter(defensiveTeam == 'HOU')
IND_data <- ready_master  |> filter(defensiveTeam == 'IND')
TEN_data <- ready_master  |> filter(defensiveTeam == 'TEN')
JAX_data <- ready_master  |> filter(defensiveTeam == 'JAX')
#NFC East
PHI_data <- ready_master  |> filter(defensiveTeam == 'PHI')
WAS_data <- ready_master  |> filter(defensiveTeam == 'WAS')
DAL_data <- ready_master  |> filter(defensiveTeam == 'DAL')
NYG_data <- ready_master  |> filter(defensiveTeam == 'NYG')
#NFC West
ARI_data <- ready_master  |> filter(defensiveTeam == 'ARI')
SF_data <- ready_master  |> filter(defensiveTeam == 'SF')
LA_data <- ready_master  |> filter(defensiveTeam == 'LA')
STL_data <- ready_master  |> filter(defensiveTeam == 'STL')
#NFC North
DET_data <- ready_master  |> filter(defensiveTeam == 'DET')
MIN_data <- ready_master  |> filter(defensiveTeam == 'MIN')
GB_data <- ready_master  |> filter(defensiveTeam == 'GB')
CHI_data <- ready_master  |> filter(defensiveTeam == 'CHI')
#NFC South
ATL_data <- ready_master  |> filter(defensiveTeam == 'ATL')
TB_data <- ready_master  |> filter(defensiveTeam == 'TB')
NO_data <- ready_master  |> filter(defensiveTeam == 'NO')
CAR_data <- ready_master  |> filter(defensiveTeam == 'CAR')
#end code generated by Samuel
count_defs <- ready_master |> count(pff_passCoverage)
print(count_defs)
#Samuel Naive Bayes
#split off just pff_passcoverage
pass_coverage <- ready_master %>%
select(quarter, down, yardsToGo, pff_passCoverage) %>%
mutate(
down = as.factor(down),
pff_passCoverage = as.factor(pff_passCoverage)
)
# Step 2: Split the data into training and testing sets
set.seed(123)  # For reproducibility
data_split <- initial_split(pass_coverage, prop = 0.75)
train_data <- training(data_split)
test_data <- testing(data_split)
# Step 3: Create a recipe with only down and yardsToGo as predictors
recipe <- recipe(pff_passCoverage ~ down + yardsToGo, data = train_data) %>%
step_normalize(all_numeric_predictors())  # Normalize numeric predictors
# Step 4: Prepare the recipe
prepared_recipe <- prep(recipe, training = train_data)
# Step 5: Bake the training and test data
x_train <- bake(prepared_recipe, new_data = NULL)  # Training data
x_test <- bake(prepared_recipe, new_data = test_data)  # Test data
# Step 6: Extract target variable for training
y_train <- x_train$pff_passCoverage  # Target variable
# Remove the target variable from the training feature set
x_train <- x_train %>% select(-pff_passCoverage)
# Convert the training data to a matrix format suitable for multinomial_naive_bayes
x_train_matrix <- model.matrix(~ . - 1, data = x_train)  # Convert to matrix, exclude intercept
x_test_matrix <- model.matrix(~ . - 1, data = x_test)    # Convert test data similarly
# Step 7: Fit the Multinomial Naive Bayes model using naivebayes::multinomial_naive_bayes
model <- naivebayes::multinomial_naive_bayes(x = x_train_matrix, y = y_train, laplace = 1)
# Step 8: Make predictions on the test set
predictions <- predict(model, newdata = x_test_matrix)
# Step 9: Evaluate the model
confusion_matrix <- table(predictions, x_test$pff_passCoverage)
print(confusion_matrix)
# Optional: Calculate accuracy
accuracy <- sum(predictions == x_test$pff_passCoverage) / nrow(x_test)
print(paste("Accuracy:", round(accuracy, 2)))
print(x_train)
#split off just pff_passcoverage
pass_coverage <- ready_master %>%
select(quarter, down, yardsToGo, pff_passCoverage) %>%
mutate(
quarter = as.factor(quarter)
down = as.factor(down),
#split off just pff_passcoverage
pass_coverage <- ready_master %>%
select(quarter, down, yardsToGo, pff_passCoverage) %>%
mutate(
quarter = as.factor(quarter),
down = as.factor(down),
pff_passCoverage = as.factor(pff_passCoverage)
)
# Step 2: Split the data into training and testing sets
set.seed(123)  # For reproducibility
data_split <- initial_split(pass_coverage, prop = 0.75)
train_data <- training(data_split)
test_data <- testing(data_split)
# Step 3: Create a recipe with only down and yardsToGo as predictors
recipe <- recipe(pff_passCoverage ~ down + yardsToGo, data = train_data) %>%
step_normalize(all_numeric_predictors())  # Normalize numeric predictors
# Step 4: Prepare the recipe
prepared_recipe <- prep(recipe, training = train_data)
# Step 5: Bake the training and test data
x_train <- bake(prepared_recipe, new_data = NULL)  # Training data
x_test <- bake(prepared_recipe, new_data = test_data)  # Test data
# Step 6: Extract target variable for training
y_train <- x_train$pff_passCoverage  # Target variable
# Remove the target variable from the training feature set
x_train <- x_train %>% select(-pff_passCoverage)
# Convert the training data to a matrix format suitable for multinomial_naive_bayes
x_train_matrix <- model.matrix(~ . - 1, data = x_train)  # Convert to matrix, exclude intercept
x_test_matrix <- model.matrix(~ . - 1, data = x_test)    # Convert test data similarly
# Step 7: Fit the Multinomial Naive Bayes model using naivebayes::multinomial_naive_bayes
model <- naivebayes::multinomial_naive_bayes(x = x_train_matrix, y = y_train, laplace = 1)
# Step 8: Make predictions on the test set
predictions <- predict(model, newdata = x_test_matrix)
# Step 9: Evaluate the model
confusion_matrix <- table(predictions, x_test$pff_passCoverage)
print(confusion_matrix)
# Optional: Calculate accuracy
accuracy <- sum(predictions == x_test$pff_passCoverage) / nrow(x_test)
print(paste("Accuracy:", round(accuracy, 2)))
#split off just pff_passcoverage
pass_coverage <- ready_master %>%
select(quarter, down, yardsToGo, pff_passCoverage) %>%
mutate(
quarter = as.factor(quarter),
down = as.factor(down),
pff_passCoverage = as.factor(pff_passCoverage)
)
# Step 2: Split the data into training and testing sets
set.seed(123)  # For reproducibility
data_split <- initial_split(pass_coverage, prop = 0.75)
train_data <- training(data_split)
test_data <- testing(data_split)
# Step 3: Create a recipe with only down and yardsToGo as predictors
recipe <- recipe(pff_passCoverage ~ down + yardsToGo, data = train_data)
# Step 4: Prepare the recipe
prepared_recipe <- prep(recipe, training = train_data)
# Step 5: Bake the training and test data
x_train <- bake(prepared_recipe, new_data = NULL)  # Training data
x_test <- bake(prepared_recipe, new_data = test_data)  # Test data
# Step 6: Extract target variable for training
y_train <- x_train$pff_passCoverage  # Target variable
# Remove the target variable from the training feature set
x_train <- x_train %>% select(-pff_passCoverage)
# Convert the training data to a matrix format suitable for multinomial_naive_bayes
x_train_matrix <- model.matrix(~ . - 1, data = x_train)  # Convert to matrix, exclude intercept
x_test_matrix <- model.matrix(~ . - 1, data = x_test)    # Convert test data similarly
# Step 7: Fit the Multinomial Naive Bayes model using naivebayes::multinomial_naive_bayes
model <- naivebayes::multinomial_naive_bayes(x = x_train_matrix, y = y_train, laplace = 1)
# Step 8: Make predictions on the test set
predictions <- predict(model, newdata = x_test_matrix)
# Step 9: Evaluate the model
confusion_matrix <- table(predictions, x_test$pff_passCoverage)
print(confusion_matrix)
# Optional: Calculate accuracy
accuracy <- sum(predictions == x_test$pff_passCoverage) / nrow(x_test)
print(paste("Accuracy:", round(accuracy, 2)))
