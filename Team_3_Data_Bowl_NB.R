#Team 3:Samuel Hartmann, Logan McDavid, Brence Moore, Nathan Lamb

#Code for reading in and sorting data written by Samuel
#load tidyverse and plays.csv
library(tidyverse)
library(tidymodels)
library(discrim)
master_data <- read.csv("plays.csv")

#Select only gameId, playID, quarter, down, yardsToGo, defensiveTeam, gameClock, pff_passCoverage, and pff_manZone
working_data <- master_data |> select(gameId, playId, quarter, down, yardsToGo, defensiveTeam, gameClock, pff_passCoverage, pff_manZone)
summary(working_data)

#Review Na and Other Values
Na_data <- master_data |> filter(is.na(pff_manZone))
Other_data <- master_data |> filter(pff_manZone == 'Other')
print(Na_data$playDescription)
print(Other_data$playDescription)

#remove row with NA values
cleaned_data <- na.omit(working_data)

#remove other values
cleaned_master <- cleaned_data |> filter(pff_manZone != 'Other')
summary(cleaned_master)

#get counts of each defensive alignment
count_defs <- cleaned_master |> count(pff_passCoverage)
print(count_defs)

#clean Cover-3 of variants and Cover-1 of Cover-1 Double
ready_master <- cleaned_master |> mutate(pff_passCoverage = case_when(
  pff_passCoverage == "Cover-3 Cloud Left" ~ "Cover-3",
  pff_passCoverage == "Cover-3 Cloud Right" ~ "Cover-3",
  pff_passCoverage == "Cover-3 Double Cloud" ~ "Cover-3",
  pff_passCoverage == "Cover-1 Double" ~ "Cover-1",
  TRUE ~ pff_passCoverage))

#get counts of each defensive alignment after cleaning
count_defs <- ready_master |> count(pff_passCoverage)
print(count_defs)

#get list of team Ids
team_Ids <- unique(ready_master$defensiveTeam)
print(team_Ids)

#AFC East
BUF_data <- ready_master  |> filter(defensiveTeam == 'BUF')
MIA_data <- ready_master  |> filter(defensiveTeam == 'MIA')
NJY_data <- ready_master  |> filter(defensiveTeam == 'NYJ')
NE_data <- ready_master  |> filter(defensiveTeam == 'NE')

#AFC West
KC_data <- ready_master  |> filter(defensiveTeam == 'KC')
LAC_data <- ready_master  |> filter(defensiveTeam == 'LAC')
DEN_data <- ready_master  |> filter(defensiveTeam == 'DEN')
LV_data <- ready_master  |> filter(defensiveTeam == 'LV')

#AFC North
PIT_data <- ready_master  |> filter(defensiveTeam == 'PIT')
BAL_data <- ready_master  |> filter(defensiveTeam == 'BAL')
CIN_data <- ready_master  |> filter(defensiveTeam == 'CIN')
CLE_data <- ready_master  |> filter(defensiveTeam == 'CLE')

#AFC South
HOU_data <- ready_master  |> filter(defensiveTeam == 'HOU')
IND_data <- ready_master  |> filter(defensiveTeam == 'IND')
TEN_data <- ready_master  |> filter(defensiveTeam == 'TEN')
JAX_data <- ready_master  |> filter(defensiveTeam == 'JAX')

#NFC East
PHI_data <- ready_master  |> filter(defensiveTeam == 'PHI')
WAS_data <- ready_master  |> filter(defensiveTeam == 'WAS')
DAL_data <- ready_master  |> filter(defensiveTeam == 'DAL')
NYG_data <- ready_master  |> filter(defensiveTeam == 'NYG')

#NFC West
ARI_data <- ready_master  |> filter(defensiveTeam == 'ARI')
SF_data <- ready_master  |> filter(defensiveTeam == 'SF')
LA_data <- ready_master  |> filter(defensiveTeam == 'LA')
STL_data <- ready_master  |> filter(defensiveTeam == 'STL')

#NFC North
DET_data <- ready_master  |> filter(defensiveTeam == 'DET')
MIN_data <- ready_master  |> filter(defensiveTeam == 'MIN')
GB_data <- ready_master  |> filter(defensiveTeam == 'GB')
CHI_data <- ready_master  |> filter(defensiveTeam == 'CHI')

#NFC South
ATL_data <- ready_master  |> filter(defensiveTeam == 'ATL')
TB_data <- ready_master  |> filter(defensiveTeam == 'TB')
NO_data <- ready_master  |> filter(defensiveTeam == 'NO')
CAR_data <- ready_master  |> filter(defensiveTeam == 'CAR')
#end code generated by Samuel

count_defs <- ready_master |> count(pff_passCoverage)
print(count_defs)
#Samuel Naive Bayes

#split off just pff_passcoverage
pass_coverage <- ready_master %>%
  select(quarter, down, yardsToGo, pff_passCoverage) %>%
  mutate(
    quarter = as.factor(quarter),
    down = as.factor(down),
    pff_passCoverage = as.factor(pff_passCoverage)
  )

# Step 2: Split the data into training and testing sets
set.seed(123)  # For reproducibility
data_split <- initial_split(pass_coverage, prop = 0.75)
train_data <- training(data_split)
test_data <- testing(data_split)

# Step 3: Create a recipe with only down and yardsToGo as predictors
recipe <- recipe(pff_passCoverage ~ down + yardsToGo, data = train_data)

# Step 4: Prepare the recipe
prepared_recipe <- prep(recipe, training = train_data)

# Step 5: Bake the training and test data
x_train <- bake(prepared_recipe, new_data = NULL)  # Training data
x_test <- bake(prepared_recipe, new_data = test_data)  # Test data

# Step 6: Extract target variable for training
y_train <- x_train$pff_passCoverage  # Target variable

# Remove the target variable from the training feature set
x_train <- x_train %>% select(-pff_passCoverage)

# Convert the training data to a matrix format suitable for multinomial_naive_bayes
x_train_matrix <- model.matrix(~ . - 1, data = x_train)  # Convert to matrix, exclude intercept
x_test_matrix <- model.matrix(~ . - 1, data = x_test)    # Convert test data similarly

# Step 7: Fit the Multinomial Naive Bayes model using naivebayes::multinomial_naive_bayes
model <- naivebayes::multinomial_naive_bayes(x = x_train_matrix, y = y_train, laplace = 1)

# Step 8: Make predictions on the test set
predictions <- predict(model, newdata = x_test_matrix)

# Step 9: Evaluate the model
confusion_matrix <- table(predictions, x_test$pff_passCoverage)
print(confusion_matrix)

# Optional: Calculate accuracy
accuracy <- sum(predictions == x_test$pff_passCoverage) / nrow(x_test)
print(paste("Accuracy:", round(accuracy, 2)))